{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3071: DtypeWarning: Columns (746,835) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\Public\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3071: DtypeWarning: Columns (700,731,740,752,761,789,811,820,829,841,850) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('C:/Users/wycliffe pepela/Desktop/projo/future/Untitled Folder/seatlite/Train.csv')\n",
    "df_test = pd.read_csv('C:/Users/wycliffe pepela/Desktop/projo/future/Untitled Folder/seatlite/Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17521, 2395), (20442, 2394))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Col1</th>\n",
       "      <th>Col2</th>\n",
       "      <th>Col3</th>\n",
       "      <th>Col4</th>\n",
       "      <th>Col5</th>\n",
       "      <th>Col6</th>\n",
       "      <th>Col7</th>\n",
       "      <th>Col8</th>\n",
       "      <th>Col9</th>\n",
       "      <th>Col10</th>\n",
       "      <th>...</th>\n",
       "      <th>Col2388</th>\n",
       "      <th>Col2389</th>\n",
       "      <th>Col2390</th>\n",
       "      <th>Col2391</th>\n",
       "      <th>Col2392</th>\n",
       "      <th>Col2393</th>\n",
       "      <th>Col2394</th>\n",
       "      <th>Col2395</th>\n",
       "      <th>Col2396</th>\n",
       "      <th>Col2397</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RIGHGSN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.086711e+04</td>\n",
       "      <td>8.648345e+03</td>\n",
       "      <td>1.576618e+04</td>\n",
       "      <td>2.890466e+04</td>\n",
       "      <td>5.273655e+03</td>\n",
       "      <td>2.346153e+04</td>\n",
       "      <td>1.285597e+04</td>\n",
       "      <td>2.871509e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>214.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RIMHI5ZGP</td>\n",
       "      <td>0</td>\n",
       "      <td>2.584312e+05</td>\n",
       "      <td>2.179633e+05</td>\n",
       "      <td>1.866287e+05</td>\n",
       "      <td>1.866287e+05</td>\n",
       "      <td>2.154519e+05</td>\n",
       "      <td>1.923615e+05</td>\n",
       "      <td>1.726538e+05</td>\n",
       "      <td>1.666974e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RIHOG1SZU</td>\n",
       "      <td>0</td>\n",
       "      <td>1.459355e+07</td>\n",
       "      <td>1.260060e+07</td>\n",
       "      <td>1.100315e+07</td>\n",
       "      <td>1.010315e+07</td>\n",
       "      <td>1.879882e+07</td>\n",
       "      <td>1.514035e+07</td>\n",
       "      <td>5.320778e+06</td>\n",
       "      <td>6.906324e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>378.0</td>\n",
       "      <td>378.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RIHOLMQWU</td>\n",
       "      <td>0</td>\n",
       "      <td>6.727043e+05</td>\n",
       "      <td>8.181116e+05</td>\n",
       "      <td>6.794893e+05</td>\n",
       "      <td>6.794893e+05</td>\n",
       "      <td>8.257254e+05</td>\n",
       "      <td>7.605803e+05</td>\n",
       "      <td>3.577134e+05</td>\n",
       "      <td>4.236612e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>702.0</td>\n",
       "      <td>702.0</td>\n",
       "      <td>644.0</td>\n",
       "      <td>951.0</td>\n",
       "      <td>951.0</td>\n",
       "      <td>743.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RIHO584ET</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.218742e+06</td>\n",
       "      <td>-3.005361e+06</td>\n",
       "      <td>-1.666241e+05</td>\n",
       "      <td>-2.403574e+06</td>\n",
       "      <td>2.583183e+06</td>\n",
       "      <td>2.086546e+06</td>\n",
       "      <td>5.938358e+06</td>\n",
       "      <td>5.446532e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1305.0</td>\n",
       "      <td>1305.0</td>\n",
       "      <td>1211.0</td>\n",
       "      <td>2205.0</td>\n",
       "      <td>2205.0</td>\n",
       "      <td>1831.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2395 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Col1  Col2          Col3          Col4          Col5          Col6  \\\n",
       "0    RIGHGSN     1  1.086711e+04  8.648345e+03  1.576618e+04  2.890466e+04   \n",
       "1  RIMHI5ZGP     0  2.584312e+05  2.179633e+05  1.866287e+05  1.866287e+05   \n",
       "2  RIHOG1SZU     0  1.459355e+07  1.260060e+07  1.100315e+07  1.010315e+07   \n",
       "3  RIHOLMQWU     0  6.727043e+05  8.181116e+05  6.794893e+05  6.794893e+05   \n",
       "4  RIHO584ET     0 -1.218742e+06 -3.005361e+06 -1.666241e+05 -2.403574e+06   \n",
       "\n",
       "           Col7          Col8          Col9         Col10  ...  Col2388  \\\n",
       "0  5.273655e+03  2.346153e+04  1.285597e+04  2.871509e+04  ...        0   \n",
       "1  2.154519e+05  1.923615e+05  1.726538e+05  1.666974e+05  ...        0   \n",
       "2  1.879882e+07  1.514035e+07  5.320778e+06  6.906324e+06  ...        0   \n",
       "3  8.257254e+05  7.605803e+05  3.577134e+05  4.236612e+05  ...        0   \n",
       "4  2.583183e+06  2.086546e+06  5.938358e+06  5.446532e+06  ...        0   \n",
       "\n",
       "   Col2389  Col2390  Col2391  Col2392  Col2393  Col2394  Col2395  Col2396  \\\n",
       "0        1        0        1    214.0    214.0     68.0     22.0     22.0   \n",
       "1        0        0        0     41.0     41.0     38.0     12.0     12.0   \n",
       "2        1        1        0    378.0    378.0    310.0     21.0     21.0   \n",
       "3        0        0        0    702.0    702.0    644.0    951.0    951.0   \n",
       "4        0        0        0   1305.0   1305.0   1211.0   2205.0   2205.0   \n",
       "\n",
       "   Col2397  \n",
       "0      7.0  \n",
       "1     11.0  \n",
       "2     16.0  \n",
       "3    743.0  \n",
       "4   1831.0  \n",
       "\n",
       "[5 rows x 2395 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Col1</th>\n",
       "      <th>Col3</th>\n",
       "      <th>Col4</th>\n",
       "      <th>Col5</th>\n",
       "      <th>Col6</th>\n",
       "      <th>Col7</th>\n",
       "      <th>Col8</th>\n",
       "      <th>Col9</th>\n",
       "      <th>Col10</th>\n",
       "      <th>Col11</th>\n",
       "      <th>...</th>\n",
       "      <th>Col2388</th>\n",
       "      <th>Col2389</th>\n",
       "      <th>Col2390</th>\n",
       "      <th>Col2391</th>\n",
       "      <th>Col2392</th>\n",
       "      <th>Col2393</th>\n",
       "      <th>Col2394</th>\n",
       "      <th>Col2395</th>\n",
       "      <th>Col2396</th>\n",
       "      <th>Col2397</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RIGD58ZWD</td>\n",
       "      <td>8.167140e+04</td>\n",
       "      <td>8.614542e+04</td>\n",
       "      <td>7.532296e+04</td>\n",
       "      <td>7.532296e+04</td>\n",
       "      <td>1.125174e+05</td>\n",
       "      <td>1.002778e+05</td>\n",
       "      <td>2.038915e+04</td>\n",
       "      <td>3.053853e+04</td>\n",
       "      <td>8.395911e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RIH660YDS</td>\n",
       "      <td>-1.362824e+07</td>\n",
       "      <td>-4.493043e+06</td>\n",
       "      <td>-3.777257e+06</td>\n",
       "      <td>-4.493043e+06</td>\n",
       "      <td>-4.749764e+06</td>\n",
       "      <td>-4.749764e+06</td>\n",
       "      <td>1.147556e+07</td>\n",
       "      <td>1.147556e+07</td>\n",
       "      <td>1.272055e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>680.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RIH660Q96</td>\n",
       "      <td>1.302700e+06</td>\n",
       "      <td>1.341449e+06</td>\n",
       "      <td>1.240085e+06</td>\n",
       "      <td>1.341449e+06</td>\n",
       "      <td>1.477503e+06</td>\n",
       "      <td>1.477503e+06</td>\n",
       "      <td>3.904086e+05</td>\n",
       "      <td>3.904086e+05</td>\n",
       "      <td>4.979106e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RIYDO15W1</td>\n",
       "      <td>1.932258e+05</td>\n",
       "      <td>1.454433e+05</td>\n",
       "      <td>1.929148e+05</td>\n",
       "      <td>1.454433e+05</td>\n",
       "      <td>1.318250e+04</td>\n",
       "      <td>1.318250e+04</td>\n",
       "      <td>8.650108e+04</td>\n",
       "      <td>8.650108e+04</td>\n",
       "      <td>5.896693e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RIYBGC1ZD</td>\n",
       "      <td>3.913385e+05</td>\n",
       "      <td>1.987337e+05</td>\n",
       "      <td>1.703432e+05</td>\n",
       "      <td>1.987337e+05</td>\n",
       "      <td>3.046536e+05</td>\n",
       "      <td>3.046536e+05</td>\n",
       "      <td>2.528307e+05</td>\n",
       "      <td>2.528307e+05</td>\n",
       "      <td>3.034449e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2394 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Col1          Col3          Col4          Col5          Col6  \\\n",
       "0  RIGD58ZWD  8.167140e+04  8.614542e+04  7.532296e+04  7.532296e+04   \n",
       "1  RIH660YDS -1.362824e+07 -4.493043e+06 -3.777257e+06 -4.493043e+06   \n",
       "2  RIH660Q96  1.302700e+06  1.341449e+06  1.240085e+06  1.341449e+06   \n",
       "3  RIYDO15W1  1.932258e+05  1.454433e+05  1.929148e+05  1.454433e+05   \n",
       "4  RIYBGC1ZD  3.913385e+05  1.987337e+05  1.703432e+05  1.987337e+05   \n",
       "\n",
       "           Col7          Col8          Col9         Col10         Col11  ...  \\\n",
       "0  1.125174e+05  1.002778e+05  2.038915e+04  3.053853e+04  8.395911e+04  ...   \n",
       "1 -4.749764e+06 -4.749764e+06  1.147556e+07  1.147556e+07  1.272055e+07  ...   \n",
       "2  1.477503e+06  1.477503e+06  3.904086e+05  3.904086e+05  4.979106e+05  ...   \n",
       "3  1.318250e+04  1.318250e+04  8.650108e+04  8.650108e+04  5.896693e+03  ...   \n",
       "4  3.046536e+05  3.046536e+05  2.528307e+05  2.528307e+05  3.034449e+05  ...   \n",
       "\n",
       "   Col2388  Col2389  Col2390  Col2391  Col2392  Col2393  Col2394  Col2395  \\\n",
       "0        0        0        0        0     44.0     44.0     23.0     41.0   \n",
       "1        1        1        2        2   1234.0   1234.0    680.0    154.0   \n",
       "2        0        0        0        0     33.0     24.0     19.0      3.0   \n",
       "3        0        0        0        0      8.0      8.0      3.0      7.0   \n",
       "4        0        0        0        0      7.0      7.0      7.0      6.0   \n",
       "\n",
       "   Col2396  Col2397  \n",
       "0     41.0     36.0  \n",
       "1    153.0     86.0  \n",
       "2      1.0      1.0  \n",
       "3      7.0      3.0  \n",
       "4      6.0      5.0  \n",
       "\n",
       "[5 rows x 2394 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop(['Col1','Col2'], axis=1)\n",
    "y = df_train.Col2\n",
    "\n",
    "XTest = df_test.drop(['Col1'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    89.949204\n",
       "1    10.050796\n",
       "Name: Col2, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17521 entries, 0 to 17520\n",
      "Columns: 2393 entries, Col3 to Col2397\n",
      "dtypes: float64(844), int64(1547), object(2)\n",
      "memory usage: 319.9+ MB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20442 entries, 0 to 20441\n",
      "Columns: 2393 entries, Col3 to Col2397\n",
      "dtypes: float64(837), int64(1545), object(11)\n",
      "memory usage: 373.2+ MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "XTest.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Col747\n",
      "[nan '-' '0' '1' 0.0 5.0 '3']\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Col836\n",
      "[nan '-' '0' 0.0 '5']\n",
      "--------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col in X.select_dtypes('object').columns:\n",
    "    print(col)\n",
    "    print(X[col].unique())\n",
    "    print(\"--------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Col702\n",
      "[nan 0.0 '0' '-']\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Col733\n",
      "[nan 0.349927106 0.23489684 0.238792569 0.326225572 -0.209956264\n",
      " 0.06998542099999999 0.283473355 0.100574803 -0.094491118 0.43723953 0.0\n",
      " 0.482371579 -0.223606798 -0.230936402 -0.287181807 0.365962527\n",
      " 0.188982237 -0.0054591330000000006 0.350883835 -0.122353127 0.203411286\n",
      " -0.124833495 -1.11e-17 0.069161739 0.209956264 0.223606798 0.490233289\n",
      " 0.368467088 0.6 -0.2 0.111762013 0.461541869 -5.55e-18 0.267600457\n",
      " 0.35689082200000005 -0.6 -3.97e-18 -0.120203624 0.2 0.447213596\n",
      " 0.338943084 -0.164954156 0.399347441 0.039904182 -0.42405191 0.356429936\n",
      " 0.094491118 0.428811668 -0.06998542099999999 0.456012469 -0.063718173\n",
      " -0.188982237 -0.12979131900000002 -0.147929864 0.13703048 -0.216598788\n",
      " 0.385177413 0.385473461 0.401123362 0.442626668 0.013413513 -0.582413089\n",
      " 0.154402328 0.206839799 -0.22174589600000005 -0.600004941 -0.565007326\n",
      " -0.000361136 -0.6890215609999999 0.518592677 -0.43511375 0.221432627\n",
      " 0.245651842 -0.575288225 0.721829742 0.162276563 0.374512042 -0.447213595\n",
      " 0.451753952 -0.225152158 0.310214176 0.242267199 0.444822876 0.442792813\n",
      " 0.219336039 0.004476371 -0.073018717 -0.122093277 0.221313334 0.400891863\n",
      " 0.436383681 0.35377942100000004 -0.189821657 -0.08895808300000001\n",
      " 0.371403294 0.346410162 0.16468313699999998 0.62543405 0.490281585\n",
      " -0.258962403 0.226582987 -0.230108124 0.196581305 0.022535721\n",
      " -0.054852857000000005 -0.227993101 0.418335577 -0.02756296 -0.121201531\n",
      " 0.111510125 0.452843054 0.5477225579999999 0.177337966 0.492150755\n",
      " -0.277709693 0.189799158 0.282758635 0.408258145 -0.058424578 0.168424834\n",
      " -0.21247584100000005 0.34413741200000003 0.149708389 0.146671727\n",
      " 0.173634762 0.34956932700000004 -0.063921486 -0.15670234800000002\n",
      " -0.138341135 0.150926773 0.359094139 0.124795216 -0.041725071 0.469476478\n",
      " 0.506661965 -0.229534318 0.446132095 -0.225366519 0.130876664 0.489897949\n",
      " 0.692820323 0.565673965 0.553748659 0.455507424 0.415391192 0.441540827\n",
      " 0.417917455 0.306362952 -0.096257884 0.358025184 '0.223606798'\n",
      " '-0.06998542099999999' '0.734026686' '-0.2' '-0.099085423' '0.447213596'\n",
      " '0.15511161' '-' '0' '5' '1' '0.0' '7']\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Col742\n",
      "['0' nan '-' '1' '3' '7' '0.305620332' '0.349927106' '0.447213596'\n",
      " '0.205395959' '-0.2' '0.223606798' 0.223606798 -0.209956264 -0.122938478\n",
      " -0.06998542099999999 -0.283473355 0.209956264 -0.136930639 -0.188982237\n",
      " 3.97e-18 0.349927106 -0.349927106 -0.447213595 -0.6 0.06998542099999999\n",
      " 0.402603716 0.188982237 -0.076696499 0.5606419979999999 -0.223606798\n",
      " 0.094491118 -0.094491118 -0.2 -0.215431759 -0.007523264 -0.19931998\n",
      " -0.226960325 -3.97e-18 -0.231311001 0.447213596 0.6 -0.554105809\n",
      " 0.579500557 -0.17689318899999998 0.2 0.16464052099999998 0.16776615 0.0\n",
      " -1.11e-17 -0.211671968 -0.037986859 -0.221313334 0.331970001 0.283473355\n",
      " -0.365962527 0.07319250599999999 -0.017498297 -0.397652424 -0.15262449\n",
      " -0.124837557 -0.3756432770000001 -6.34e-18 -0.093659379 -0.029186341\n",
      " 0.06796030900000001 0.347415301 -0.18257418600000005 0.02998002\n",
      " -0.287494454 -0.450341001 0.018196863 -0.194166363 0.346498814\n",
      " -0.442626668 -0.155632306 0.041226318 -0.186866574 0.220384484\n",
      " 0.365962527 -0.073192505 0.315408052 -0.081467785 0.20734176 -0.124832824\n",
      " 0.002257594 -0.114875814 -0.104944245 -0.23414936600000005 0.097267035\n",
      " -0.150610086 1.11e-17 -0.659735063 -0.238735071 -0.5477225579999999\n",
      " -0.734846923 0.038240013 5.55e-18 0.475040067]\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Col754\n",
      "[nan 0.255550626 0.0 0.12995089 0.109986363 0.185576872 -0.209898855\n",
      " 0.06998542099999999 0.159719141 -0.001457966 -0.13122268 0.040406103\n",
      " -7.93e-18 0.49481681 -0.22360674100000005 -0.229849969 0.233550383\n",
      " 0.246248152 0.131222665 -0.223606798 -0.375290496 -0.136507119\n",
      " -0.052743795 -0.217642129 -1.11e-17 0.165375297 0.209956264 0.223606814\n",
      " -0.094491361 -0.029918943 0.380055619 0.223606798 0.141421356\n",
      " -0.199999983 0.143624108 0.289577449 0.057140606 -0.099999999 0.194040438\n",
      " 0.057735027 0.193081132 -0.6 3.97e-18 -0.153186077 -2.73e-08 0.447213608\n",
      " -0.101577553 0.131223026 0.13122267199999998 0.050194171 0.14580145\n",
      " -0.199999922 0.223606744 -0.223606799 0.3 0.138646785 0.181140652\n",
      " 0.114640687 -0.6964965240000001 0.421185266 0.131222458 0.070710678\n",
      " -0.131090431 -0.127775312 0.451888684 -0.098668689 0.13122267599999998\n",
      " -0.3 -0.18898223100000006 0.145802961 0.068942204 0.131222668 0.349927106\n",
      " -0.330056636 0.254824196 -0.1 -0.255550626 -0.2 0.127775312 0.199999999\n",
      " -0.05714285700000001 -0.166047866 0.405486387 -0.001940278 0.298170133\n",
      " 0.350947558 0.1 -0.094904459 0.218704435 0.080812204 0.058321185\n",
      " -0.066436405 0.166852747 0.039700553 -0.692820323 0.173206903 0.6\n",
      " -0.600010001 -0.552004404 -0.010453782 -0.67966168 0.383933528\n",
      " -0.113492065 0.013320486 0.105960894 -0.421381165 -0.390197011\n",
      " -0.194943164 0.071428571 -0.094491118 -0.069985445 -0.063887656\n",
      " -0.13122245300000002 0.386359757 0.031943828 -0.188982234 -0.346410158\n",
      " -0.218704439 -0.447213596 0.472842271 -0.0057475880000000005 0.092788436\n",
      " -0.001277644 0.339417751 0.296479211 -0.556013633 -0.043739887000000005\n",
      " 0.162165679 -0.067285409 0.324642883 0.28867513300000003 -0.196748986\n",
      " 0.223606797 0.239009243 0.283473355 0.373315794 -0.13122273199999998\n",
      " 0.4254486220000001 0.283472145 0.131808223 0.358578034 0.009803556\n",
      " -0.30609731100000004 0.346410162 -0.053688733 0.223606771 0.207634884\n",
      " 0.415253839 0.218687164 -0.113005642 0.10714421 0.722359427 0.218704428\n",
      " 0.604115999 0.07985956799999999 0.188589872 0.159719133 -0.094491112\n",
      " 0.094491121 0.299999998 0.189289535 -0.13122266400000002 -0.065990722\n",
      " -0.223604406 0.218704453 0.13122266400000002 -0.312846525 0.03168801\n",
      " 0.108248334 -0.133071912 6.34e-18 -0.237017268 0.550576821 0.218704429\n",
      " 0.080757012 -0.06974375299999999 -0.128239321 0.043740942000000005\n",
      " 0.461880216 0.051295197 0.28571428600000004 0.218704447 0.349927366 0.2\n",
      " 0.255550617 -0.218704447 0.043740887 0.416016597 0.464224494 -0.092790942\n",
      " -0.164957229 -0.036379371 -0.10222025 0.103752677 0.065947029 0.025253814\n",
      " 0.21429761 0.240170637 0.283473535 0.234382973 -0.07325649599999999\n",
      " -0.219069077 0.074673276 -0.06684286 0.34992708 0.188980898\n",
      " 0.18898223600000005 0.33691136 -0.100000001 2.53e-08 -0.12777531\n",
      " 0.28333797 -0.114285715 -0.199999488 0.3660460920000001 -0.093631335\n",
      " -0.127598215 -0.194360455 0.429171836 0.095831456 0.111846834 -3.97e-18\n",
      " -0.025253814 -0.22360702 0.043740108 0.043740868 0.16613683\n",
      " 0.043740990999999986 0.209037071 0.274624523 0.202599204 0.299999946\n",
      " 0.256059871 0.447213635 0.4878193320000001 0.364479774 0.297902104\n",
      " 0.515483 -0.300876702 -0.06998542099999999 0.26747405 -0.306974687\n",
      " 0.190038669 0.486743494 -0.100009215 -0.141421356 0.378836728 0.067324969\n",
      " -0.209956264 0.114285724 0.18301787100000005 0.563771675 0.162728366\n",
      " 0.409904167 0.140881406 0.178885438 0.255550703 0.234983762\n",
      " 0.33928893600000004 -0.298363457 0.127775314 -0.25555063 -0.247263164\n",
      " '0.0' '0.1' '-0.127775313' '-0.67291248' '-0.200000055' '-0.2'\n",
      " '-0.13399376300000002' '0.300000004' '0.15326104699999998' '-0.127775316'\n",
      " '-' '0' '2' '1' '6' '4']\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Col763\n",
      "['0' nan '-' '1' '0.326166446' '0.255548809' '0.447213596' '0.214222192'\n",
      " '-0.19999999300000001' '0.1' 0.0 0.22360653100000005 -0.255550632\n",
      " -0.228996851 -0.06998542099999999 -0.291605922 -0.131965775 -0.283473354\n",
      " 0.127775313 -0.016694786 -0.188982225 -0.107142857 0.349927107\n",
      " 0.349927106 -0.349927106 0.223606798 -0.447213594 -0.6 -0.447213596\n",
      " 0.06998542099999999 -0.18898223600000005 -0.283473355 0.446235617\n",
      " -0.188982237 -0.342857143 0.188982237 -0.143747227 0.069985422\n",
      " 0.332215814 -0.28571428600000004 -0.200261591 -0.25555057600000003\n",
      " -0.447213595 -0.349927111 -0.255550626 0.343375854 0.223606794\n",
      " -0.131965777 -0.300000002 -0.3 -0.209956264 0.16071428699999998\n",
      " 0.209956261 0.209956264 -0.094491118 -0.10222025 -0.199999992\n",
      " -0.127775313 0.099999845 -0.159008126 -0.223606798 -0.228878618\n",
      " -0.240645609 -0.281105689 -0.294338033 -0.043740886 -0.357770876\n",
      " -0.228571429 0.142290492 -0.05714285700000001 -0.282842712 -0.35\n",
      " 0.2236068 -0.303899303 0.3 0.600000002 -0.204440501 -0.560082407\n",
      " -6.34e-18 0.579970856 -0.141421356 0.255550626 -0.091386794 0.2\n",
      " -0.131222667 -0.188982238 -0.277025626 -0.255550624 -0.085714286\n",
      " -0.6000000010000001 0.003924003 0.6 -0.20534955 -0.188982226 -0.447213589\n",
      " 0.282842713 -0.332215814 -0.209956265 0.094491118 1.11e-17 -1.11e-17\n",
      " -0.22012065 -0.34641015 0.1 0.447213595 -0.346410161 -0.131222725\n",
      " -0.069985424 -0.031466041 -0.209956267 -0.200000001 -0.2 -0.255255593\n",
      " -0.283473352 0.369607508 0.21870444100000005 -0.197948664 0.080812204\n",
      " -0.404145188 0.447213591 -0.349927107 -0.218704421 0.447213596\n",
      " -0.349927104 -0.173205081 -0.349927105 -0.265072579 -0.115470056\n",
      " -0.043740888 -0.131222668 -0.299999985 -0.200000043 -0.107627737\n",
      " 0.21870444 -0.283473343 -0.306660751 0.114765758 0.13122266400000002\n",
      " -0.26244533 0.196428571 -0.281105688 -0.126269068 -0.346410163\n",
      " -0.349927109 -0.175691055 -0.069985423 -0.255550577 0.188982238\n",
      " 0.283473355 -0.3984237570000001 -0.166972894 -0.45 6.34e-18 -0.087481777\n",
      " -0.218704439 -0.349927125 -0.098974354 0.039906528 -0.447213613\n",
      " -0.21870444100000005 -0.145802963 -0.442626668 0.011536323 -0.128111647\n",
      " -0.171428571 -0.274866179 -0.271522539 0.349999999 1.98e-18 -0.159719141\n",
      " -0.5999999979999999 -0.27152254 0.03182277 -0.153330376\n",
      " 0.34969442799999995 -0.183224904 -0.144337567 -0.34992709200000005\n",
      " -0.424264069 0.13122266300000002 -0.032245277999999995 0.346410161\n",
      " -1.98e-18 -0.349927108 -0.447213593 -0.281367877 0.043740893 -0.447213581\n",
      " 0.447213597 0.017857143 -0.043740899000000014 -0.691386825 -0.218704443\n",
      " 0.135984727 -0.032991444 -0.191662969 -0.306894069 0.255550627\n",
      " -0.17543260800000002 -0.178571433 0.094491115 0.255550625 -0.447213603\n",
      " -0.209956263 0.126037289 -0.404145187 0.223606797 0.209956258\n",
      " -0.412754535 -0.300000001 0.043740888 -0.058321186 -0.230940108\n",
      " 0.230940105 0.461880216 0.159719138 -0.13122265 -0.248017361\n",
      " -0.14151918900000002 -0.018672721 -0.015971914 -0.094491119 -0.153033723\n",
      " 0.132415981 -0.209956125 0.043740887 -0.218704437 0.365183575\n",
      " 0.18898223100000006 0.188982205 -0.25555062 3.97e-18 -0.132040286\n",
      " 0.07985957099999999 0.346788969 -0.131222665 0.04374089 0.094491114\n",
      " -0.256187313 0.219660486 -0.120311355 -0.1 -0.255550628 -0.209956253\n",
      " -0.12675619400000002 -0.094491225 -0.043740885 0.047765595 0.173205083\n",
      " 0.28347348 -0.18954385 -0.209956245 -0.043740893 -0.255550622\n",
      " -0.276296579 -0.15 0.204440501 -0.349927096 0.131222668 -0.234525934\n",
      " -0.043740889000000005 -0.196428571 -1.27e-17 0.171428572\n",
      " -0.22401978600000005 0.349927088 -0.313897337 -0.447213601 -0.349927103\n",
      " -0.223606778 -0.20882466 0.4472135999999999 -0.338690657 0.45 -0.67606938\n",
      " -0.223606797 0.05714285700000001 0.15 -0.16900008 -0.080812254\n",
      " 0.230940108 -0.5551777729999999 -0.07069737599999999 0.34641016\n",
      " -0.593462811 -0.008106155 -0.05 0.404145189 0.542348323]\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Col791\n",
      "[nan '0' '-' 0.0]\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Col813\n",
      "[nan '-' 0.0]\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Col822\n",
      "[nan 0.349927106 0.23489684 0.239562867 0.400694318 -0.209956264\n",
      " 0.06998542099999999 -0.102382484 0.43723953 -0.06998542099999999\n",
      " 0.482371579 -0.223606798 -0.230936402 -0.287181807 0.442626668\n",
      " 0.209956264 -0.0054591330000000006 0.350883835 -0.122353127 0.16265401\n",
      " -0.124833495 0.0 0.069161739 0.223606798 0.490233289 0.368467088 0.6 -0.2\n",
      " 0.08325307400000001 0.461541869 0.267600457 0.35689082200000005 -0.6\n",
      " -0.120203624 0.2 0.447213596 0.113864771 0.338943084 -0.164954156\n",
      " 0.399347441 -0.118367822 -0.42405191 0.410778246 0.481194381 0.456012469\n",
      " -0.063718173 -0.349927106 -0.12979131900000002 -0.147929864 0.13703048\n",
      " -0.216598788 0.385177413 0.383395542 0.420777577 0.013413513 -0.582413089\n",
      " 0.067713152 0.206839799 -0.22174589600000005 1.11e-17 -0.600004941\n",
      " -0.565007326 -0.000361136 -0.6890215609999999 0.518592677 -0.43511375\n",
      " 0.221432627 0.264520028 -0.575288225 0.721829742 0.162276563 0.374512042\n",
      " -0.447213595 0.451753952 -0.225152158 0.374413459 0.230980929 0.525417737\n",
      " 0.442792813 0.219336039 -0.065321226 -0.197282777 -0.30744352 0.221313334\n",
      " 0.469476478 0.436383681 0.373613994 -0.376751084 -0.24915334100000006\n",
      " 0.371403294 0.346410162 0.07360772 0.62543405 0.490281585 -0.258962403\n",
      " 0.167270695 -0.472438504 0.196581305 0.022535721 -0.197497145 -0.44864875\n",
      " 0.45175395 -0.13298233 -0.121201531 0.088775205 -5.55e-18 0.452843054\n",
      " 0.5477225579999999 0.051904115999999986 0.492150755 -0.277709693\n",
      " 0.159159628 0.347859255 0.454588828 -0.202717544 0.163183313 0.311573088\n",
      " 0.07790271 -0.322246648 0.173634762 0.428300912 -0.228416096 -0.328803164\n",
      " -0.392041393 -0.017524828 0.41113783 0.124795216 -0.041725071 0.506661965\n",
      " -0.229534318 0.446132095 -0.225366519 0.130876664 0.489897949 0.692820323\n",
      " 0.565673965 0.553748659 0.455507424 0.415391192 0.441540827 0.417917455\n",
      " 0.306362952 -0.096257884 0.358025184 '0.223606798' '-0.06998542099999999'\n",
      " '0.734026686' '-0.2' '-0.099085423' '0.447213596' '0.15511161' '-' '0'\n",
      " '1' '5' '0.0']\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Col831\n",
      "['0' nan '-' '1' '0.305620332' '0.349927106' '0.447213596' '0.205395959'\n",
      " '-0.2' '0.223606798' 0.223606798 -0.209956264 -0.122938478\n",
      " -0.06998542099999999 0.209956264 -0.136930639 -0.349927106 0.349927106\n",
      " -0.447213595 -0.6 0.06998542099999999 0.402603716 -0.076696499\n",
      " 0.5606419979999999 -0.223606798 -0.2 -0.215431759 -0.19931998\n",
      " -0.094368323 -0.500396954 0.447213596 0.6 -0.554105809 0.579500557\n",
      " -0.17689318899999998 0.2 0.14312243 0.16776615 0.0 -5.55e-18 -0.211671968\n",
      " -0.149224801 -0.221313334 0.331970001 -9.52e-18 -0.017498297 -0.397652424\n",
      " -0.15262449 -0.124837557 -0.3756432770000001 -6.34e-18 -0.20899904\n",
      " 0.01327184 0.347415301 -0.18257418600000005 0.02998002 -0.450341001\n",
      " 0.018196863 -0.194166363 0.346498814 -0.442626668 -0.155632306\n",
      " -0.041838285 -0.377955797 0.159682534 0.442626668 0.336577004\n",
      " -0.081467785 0.20734176 -0.124832824 0.002257594 -0.305272986\n",
      " -0.229966764 -0.23414936600000005 0.097267035 -0.150610086 -1.11e-17\n",
      " -0.659735063 -0.238735071 5.55e-18 -0.5477225579999999 -0.734846923\n",
      " 0.038240013 0.475040067 1.11e-17]\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Col843\n",
      "[nan 0.255550626 0.0 0.12995089 0.175978183 0.296922995 -0.209898855\n",
      " 0.06998542099999999 -0.447010279 -0.209956267 0.040406103 -0.06998542\n",
      " 0.49481681 -0.22360674100000005 -0.229849969 0.233550383 0.393997043\n",
      " 0.209956264 -0.223606798 -0.375290496 -0.136507119 -0.247431373\n",
      " -0.217642129 0.165375297 0.223606814 -0.209956035 -0.029918943\n",
      " 0.380055619 0.223606798 0.141421356 -0.199999983 0.106219251 0.289577449\n",
      " 0.057140606 -0.099999999 0.194040438 0.057735027 0.193081132 -0.6\n",
      " -0.06998541900000001 -0.153186077 -2.73e-08 0.447213608 -0.256986263\n",
      " 0.209956459 0.050194171 0.127775215 -0.199999922 0.223606744 -0.223606799\n",
      " 0.3 0.138646785 0.181140652 0.183425099 -0.6964965240000001 0.477364493\n",
      " 0.127774575 0.070710678 -0.20974469 -0.127775312 0.451888684 -0.098668689\n",
      " 0.209956282 -0.3 -0.34992711600000004 0.127775313 0.068942204 0.209956263\n",
      " 0.349927106 -0.330056636 0.254824196 -0.1 -0.255550626 -0.2 0.127775312\n",
      " 0.199999999 -0.05714285700000001 -0.166047866 0.405486387 -0.450505381\n",
      " 0.28539847100000004 0.350947558 0.1 -0.094904459 0.349927108 0.080812204\n",
      " -0.066436405 -0.132830346 0.039700553 -0.692820323 0.173206903 0.6\n",
      " -0.600010001 -0.552004404 -0.010453782 -0.67966168 0.383933528\n",
      " -0.113492065 0.013320486 0.16953777 -0.421381165 -0.390197011\n",
      " -0.194943164 0.114285714 -0.209956264 -0.069985445 -0.171428577\n",
      " -0.20995627 0.386359757 -0.34992711 -0.346410158 -0.349927106\n",
      " -0.447213596 0.472842271 -0.0057475880000000005 0.148461498 -0.002044244\n",
      " 0.385755453 0.296479211 -0.556013633 -0.069983819 0.135496212\n",
      " -0.192562697 0.398814314 0.28867513300000003 -0.43568938 0.223606797\n",
      " 0.239009243 0.429036151 -0.209956295 0.4254486220000001 0.349927809\n",
      " 0.131808223 0.387122571 0.01568569 -0.4897556970000001 -0.209956268\n",
      " 0.346410162 -0.06998542099999999 -0.053688733 0.223606771 0.2 0.415253839\n",
      " 0.34989819 -0.180809028 0.17143073600000006 0.722359427\n",
      " 0.34992710200000005 0.604115999 0.127775308 0.188589872 0.255550613\n",
      " -0.20995624 0.06998541900000001 0.299999998 0.210452697 -0.209956554\n",
      " -0.105585156 -0.223604406 0.349927109 -0.500554284 0.03168801 0.108248334\n",
      " -0.21291506 -0.379227649 0.550576821 0.129211464 -0.111590177\n",
      " -0.128239321 0.069985437 0.461880216 0.082072313 0.28571428600000004\n",
      " 0.34992707 0.349927366 0.255550617 -0.349927104 1.11e-17 0.416016597\n",
      " 0.464224494 -0.148465506 -0.164957229 -0.036379371 -0.10222025\n",
      " 0.103752677 -0.000716277 0.040406102 0.161624407 -0.032344349 0.349925937\n",
      " 0.170806016 -0.11721042 0.349927107 0.119477242 -0.106948575 0.34992708\n",
      " 0.209954776 0.335937624 -0.100000001 2.53e-08 -0.12777531 0.349895105\n",
      " -0.114285715 -0.199999488 0.442753079 -0.214047986 -0.204157143\n",
      " -0.310976727 0.473417538 0.349927105 0.057142853 -0.35516400200000003\n",
      " -0.040406102 -0.22360702 -0.06998260099999999 0.069982449 0.069985424\n",
      " 0.265818928 0.06998544 0.334459313 0.439399221 0.202599204 0.299999946\n",
      " 0.256059871 0.447213635 0.4878193320000001 0.364479774 0.297902104\n",
      " 0.515483 -0.300876702 0.26747405 -0.306974687 0.190038669 0.486743494\n",
      " -0.100009215 -0.141421356 0.378836728 0.067324969 0.114285724\n",
      " 0.18301787100000005 0.563771675 0.162728366 0.409904167 0.140881406\n",
      " 0.178885438 0.255550703 0.234983762 0.33928893600000004 -0.298363457\n",
      " 0.127775314 -0.25555063 -0.247263164 '0.0' '0.1' '-0.127775313'\n",
      " '-0.67291248' '-0.200000055' '-0.2' '-0.13399376300000002' '0.300000004'\n",
      " '0.15326104699999998' '-0.127775316' '-' '0' '2' '1' '6']\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "Col852\n",
      "['0' nan '-' '6' 0.0 '0.326166446' '0.255548809' '0.447213596'\n",
      " '0.214222192' '-0.19999999300000001' '0.1' 0.22360653100000005\n",
      " -0.255550632 -0.228996851 -0.06998542099999999 -0.131965775 0.127775313\n",
      " -0.016694786 -0.34992697700000003 -0.171428571 0.349927107 0.349927106\n",
      " -0.349927106 0.223606798 -0.447213594 -0.6 -0.447213596\n",
      " 0.06998542099999999 0.446235617 -0.342857143 0.209956264 -0.285714287\n",
      " 0.069985422 0.332215814 -0.28571428600000004 -0.200261591\n",
      " -0.25555057600000003 -0.447213595 -0.349927111 -0.255550626 0.343375854\n",
      " 0.223606794 -0.131965777 -0.300000002 -0.3 -0.209956264 -0.349927105\n",
      " 0.164957228 0.209956261 -0.10222025 -0.199999992 -0.127775313 0.099999845\n",
      " -0.159008126 -0.223606798 -0.240645609 -0.281105689 -0.357770876\n",
      " -0.228571429 0.10448033 -0.05714285700000001 -0.282842712 -0.35 0.2236068\n",
      " -0.486238894 0.3 0.600000002 -0.204440501 -0.560082407 -6.34e-18\n",
      " 0.579970856 -0.141421356 0.255550626 -0.091386794 0.2 -0.349927035\n",
      " -0.255550624 -0.085714286 -0.6000000010000001 0.006278408 0.6 -0.20534955\n",
      " -0.34992711 -0.447213589 0.282842713 -0.332215814 -0.209956265 -5.55e-18\n",
      " -0.22012065 -0.34641015 0.1 0.447213595 -0.346410161 -0.20995628\n",
      " -0.069985424 -0.050345665 -0.209956267 -0.200000001 -0.2 -0.255255593\n",
      " 0.369607508 -0.197948664 0.080812204 -0.404145188 0.447213591\n",
      " -0.349927107 -0.349927096 0.447213596 -0.349927104 -0.173205081\n",
      " -0.281105687 -0.115470056 -0.209956263 -0.299999985 -0.200000043\n",
      " -0.17220437800000002 -0.306660751 0.114765758 0.314285714 -0.281105688\n",
      " -0.346410163 -0.349927109 -0.069985423 -0.255550577 0.209956262\n",
      " -0.3984237570000001 -0.166972894 -0.45 -0.349927125 -0.098974354\n",
      " 0.039906528 -0.447213613 -0.442626668 0.011536323 -0.204978634\n",
      " 0.349926991 0.349999999 6.34e-18 -0.314285714 -0.5999999979999999\n",
      " -0.078317505 -0.153330376 0.34969442799999995 -0.183224904 -0.230940108\n",
      " -0.34992709200000005 -0.424264069 -0.032245277999999995 0.346410161\n",
      " -0.349927108 -0.447213593 -0.447213581 0.447213597 -0.032991439\n",
      " -0.069985422 -0.691386825 0.135984727 -0.032991444 -0.306894069\n",
      " 0.255550627 -0.17543260800000002 -0.285714293 0.06998541900000001\n",
      " 0.255550625 -0.447213603 0.126037289 -0.404145187 0.223606797 0.209956258\n",
      " -0.412754535 -0.300000001 0.230940105 0.461880216 0.25555062100000003\n",
      " -0.255550455 -0.248017361 -0.14151918900000002 -0.137507711 -0.025555063\n",
      " -0.209956261 -0.341813233 0.211865569 -0.209956125 0.441454849\n",
      " 0.209956294 -0.25555062 -0.211264458 0.386139013 0.069985418 -0.256187313\n",
      " 0.219660486 -0.120311355 -0.1 0.028571429 -0.255550628 -0.209956253\n",
      " -0.12675619400000002 -0.209956382 -0.10037054 0.173205083 0.349928494\n",
      " -0.357770875 -0.209956245 -0.255550622 -0.442074528 -0.15 0.204440501\n",
      " -0.234525934 0.171428572 -0.22401978600000005 0.349927088 -0.313897337\n",
      " -0.447213601 -0.349927103 -0.223606778 -0.20882466 0.4472135999999999\n",
      " 1.11e-17 -0.338690657 0.45 -0.67606938 -0.223606797 0.05714285700000001\n",
      " 0.15 -0.16900008 -0.080812254 0.230940108 -0.5551777729999999\n",
      " -0.07069737599999999 0.34641016 -0.593462811 -0.008106155 -0.05\n",
      " 0.404145189 0.542348323]\n",
      "--------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col in XTest.select_dtypes('object').columns:\n",
    "    print(col)\n",
    "    print(XTest[col].unique())\n",
    "    print(\"--------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_float(row):\n",
    "    if row == '-':\n",
    "        return np.nan\n",
    "    else:\n",
    "        return float(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "columns_need_treatment = list(X.select_dtypes('object').columns) + list(XTest.select_dtypes('object').columns)\n",
    "print(len(columns_need_treatment))\n",
    "print(columns_need_treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in columns_need_treatment:\n",
    "    X[col] = X[col].apply(convert_to_float)\n",
    "    XTest[col] = XTest[col].apply(convert_to_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data contains 1096 duplicate rows and Test data contains 1184 duplicate rows.\n"
     ]
    }
   ],
   "source": [
    "duplicate_rows_in_train = X.duplicated()\n",
    "duplicate_rows_in_test = XTest.duplicated()\n",
    "\n",
    "print(\"Train data contains %d duplicate rows and Test data contains %d duplicate rows.\"%(sum(duplicate_rows_in_train), \n",
    "                                                                                             sum(duplicate_rows_in_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.887774\n",
       "1    0.112226\n",
       "Name: Col2, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[duplicate_rows_in_train].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1557     0\n",
       "3996     0\n",
       "3997     0\n",
       "5104     0\n",
       "5438     0\n",
       "5989     0\n",
       "6673     0\n",
       "8248     0\n",
       "8585     0\n",
       "9439     0\n",
       "9915     0\n",
       "10130    1\n",
       "10164    0\n",
       "10178    0\n",
       "10188    0\n",
       "10189    1\n",
       "10217    0\n",
       "10222    0\n",
       "10223    1\n",
       "10224    0\n",
       "10225    0\n",
       "10226    1\n",
       "10227    0\n",
       "10310    0\n",
       "10320    0\n",
       "10330    0\n",
       "10331    0\n",
       "10351    1\n",
       "10696    0\n",
       "10733    0\n",
       "Name: Col2, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y[duplicate_rows_in_train].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X['duplicate_row'] = False\n",
    "XTest['duplicate_row'] = False\n",
    "\n",
    "X.loc[duplicate_rows_in_train, 'duplicate_row'] = True\n",
    "XTest.loc[duplicate_rows_in_test, 'duplicate_row'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Col136 Col137\n",
      "Col942 Col1344\n",
      "Col943 Col1345\n",
      "Col962 Col1352\n",
      "Col963 Col1353\n",
      "Col974 Col1066\n",
      "Col974 Col1158\n",
      "Col974 Col1250\n",
      "Col974 Col1704\n",
      "Col974 Col1796\n",
      "Col974 Col1888\n",
      "Col974 Col1980\n",
      "Col975 Col1705\n",
      "Col986 Col1302\n",
      "Col987 Col1303\n",
      "Col990 Col1082\n",
      "Col991 Col1083\n",
      "Col992 Col1084\n",
      "Col992 Col1176\n",
      "Col993 Col1085\n",
      "Col1006 Col1310\n",
      "Col1007 Col1311\n",
      "Col1018 Col1110\n",
      "Col1018 Col1202\n",
      "Col1018 Col1748\n",
      "Col1018 Col1840\n",
      "Col1018 Col1932\n",
      "Col1019 Col1111\n",
      "Col1019 Col1203\n",
      "Col1019 Col1251\n",
      "Col1019 Col1295\n",
      "Col1019 Col1749\n",
      "Col1019 Col1841\n",
      "Col1019 Col1933\n",
      "Col1019 Col1981\n",
      "Col1019 Col2025\n",
      "Col1034 Col1434\n",
      "Col1035 Col1435\n",
      "Col1054 Col1442\n",
      "Col1055 Col1443\n",
      "Col1066 Col1158\n",
      "Col1066 Col1250\n",
      "Col1066 Col1704\n",
      "Col1066 Col1796\n",
      "Col1066 Col1888\n",
      "Col1066 Col1980\n",
      "Col1067 Col1159\n",
      "Col1067 Col1797\n",
      "Col1067 Col1889\n",
      "Col1078 Col1392\n",
      "Col1079 Col1393\n",
      "Col1084 Col1176\n",
      "Col1098 Col1400\n",
      "Col1099 Col1401\n",
      "Col1110 Col1202\n",
      "Col1110 Col1748\n",
      "Col1110 Col1840\n",
      "Col1110 Col1932\n",
      "Col1111 Col1203\n",
      "Col1111 Col1251\n",
      "Col1111 Col1295\n",
      "Col1111 Col1749\n",
      "Col1111 Col1841\n",
      "Col1111 Col1933\n",
      "Col1111 Col1981\n",
      "Col1111 Col2025\n",
      "Col1126 Col1524\n",
      "Col1127 Col1525\n",
      "Col1146 Col1532\n",
      "Col1147 Col1533\n",
      "Col1158 Col1250\n",
      "Col1158 Col1704\n",
      "Col1158 Col1796\n",
      "Col1158 Col1888\n",
      "Col1158 Col1980\n",
      "Col1159 Col1797\n",
      "Col1159 Col1889\n",
      "Col1170 Col1482\n",
      "Col1171 Col1483\n",
      "Col1190 Col1490\n",
      "Col1191 Col1491\n",
      "Col1202 Col1748\n",
      "Col1202 Col1840\n",
      "Col1202 Col1932\n",
      "Col1203 Col1251\n",
      "Col1203 Col1295\n",
      "Col1203 Col1749\n",
      "Col1203 Col1841\n",
      "Col1203 Col1933\n",
      "Col1203 Col1981\n",
      "Col1203 Col2025\n",
      "Col1218 Col1614\n",
      "Col1219 Col1615\n",
      "Col1238 Col1622\n",
      "Col1239 Col1623\n",
      "Col1250 Col1704\n",
      "Col1250 Col1796\n",
      "Col1250 Col1888\n",
      "Col1250 Col1980\n",
      "Col1251 Col1295\n",
      "Col1251 Col1749\n",
      "Col1251 Col1841\n",
      "Col1251 Col1933\n",
      "Col1251 Col1981\n",
      "Col1251 Col2025\n",
      "Col1262 Col1572\n",
      "Col1263 Col1573\n",
      "Col1282 Col1580\n",
      "Col1283 Col1581\n",
      "Col1294 Col2024\n",
      "Col1295 Col1749\n",
      "Col1295 Col1841\n",
      "Col1295 Col1933\n",
      "Col1295 Col1981\n",
      "Col1295 Col2025\n",
      "Col1672 Col2074\n",
      "Col1673 Col2075\n",
      "Col1692 Col2082\n",
      "Col1693 Col2083\n",
      "Col1704 Col1796\n",
      "Col1704 Col1888\n",
      "Col1704 Col1980\n",
      "Col1716 Col2032\n",
      "Col1717 Col2033\n",
      "Col1720 Col1812\n",
      "Col1721 Col1813\n",
      "Col1722 Col1814\n",
      "Col1722 Col1906\n",
      "Col1723 Col1815\n",
      "Col1736 Col2040\n",
      "Col1737 Col2041\n",
      "Col1748 Col1840\n",
      "Col1748 Col1932\n",
      "Col1749 Col1841\n",
      "Col1749 Col1933\n",
      "Col1749 Col1981\n",
      "Col1749 Col2025\n",
      "Col1764 Col2164\n",
      "Col1765 Col2165\n",
      "Col1784 Col2172\n",
      "Col1785 Col2173\n",
      "Col1796 Col1888\n",
      "Col1796 Col1980\n",
      "Col1797 Col1889\n",
      "Col1808 Col2122\n",
      "Col1809 Col2123\n",
      "Col1814 Col1906\n",
      "Col1828 Col2130\n",
      "Col1829 Col2131\n",
      "Col1840 Col1932\n",
      "Col1841 Col1933\n",
      "Col1841 Col1981\n",
      "Col1841 Col2025\n",
      "Col1856 Col2254\n",
      "Col1857 Col2255\n",
      "Col1876 Col2262\n",
      "Col1877 Col2263\n",
      "Col1888 Col1980\n",
      "Col1900 Col2212\n",
      "Col1901 Col2213\n",
      "Col1920 Col2220\n",
      "Col1921 Col2221\n",
      "Col1933 Col1981\n",
      "Col1933 Col2025\n",
      "Col1948 Col2344\n",
      "Col1949 Col2345\n",
      "Col1968 Col2352\n",
      "Col1969 Col2353\n",
      "Col1981 Col2025\n",
      "Col1992 Col2302\n",
      "Col1993 Col2303\n",
      "Col2012 Col2310\n",
      "Col2013 Col2311\n"
     ]
    }
   ],
   "source": [
    "features = X.columns\n",
    "duplicate_columns = set()\n",
    "for i in range(len(features)):\n",
    "    for j in range(i+1, len(features)):\n",
    "        if np.all(X[features[i]] == X[features[j]]):\n",
    "            print(features[i], features[j])\n",
    "            duplicate_columns.add(features[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate columns: 101\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of duplicate columns:\",len(duplicate_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected_Features : 2293\n"
     ]
    }
   ],
   "source": [
    "selected_features = [_ for _ in X.columns if _ not in duplicate_columns]\n",
    "print(\"Selected_Features :\",len(selected_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X[selected_features], y, \n",
    "                                                      random_state=RANDOM_SEED, \n",
    "                                                      test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(params):\n",
    "    try:\n",
    "\n",
    "        print(\"Training with params: \",params)\n",
    "        num_round = int(params['n_estimators'])\n",
    "        del params['n_estimators']\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "        watchlist = [(dtrain, 'train'),(dvalid, 'eval')]\n",
    "        gbm_model = xgb.train(params, dtrain, num_round,\n",
    "                              evals=watchlist,\n",
    "                              verbose_eval=False)\n",
    "        predictions = gbm_model.predict(dvalid,\n",
    "                                        ntree_limit=gbm_model.best_iteration + 1)\n",
    "        predictions = (predictions >= 0.5).astype('int')\n",
    "        score = f1_score(y_valid, predictions, average='weighted')\n",
    "        print(\"\\tScore {0}\\n\\n\".format(score))\n",
    "        \n",
    "        # The score function should return the loss (1-score)\n",
    "        # since the optimize function looks for the minimum\n",
    "        loss = 1 - score\n",
    "        return {'loss': loss, 'status': STATUS_OK}\n",
    "   \n",
    "    # In case of any exception or assertionerror making score 0, so that It can return maximum loss (ie 1)\n",
    "    except AssertionError as obj:\n",
    "        #print(\"AssertionError: \",obj)\n",
    "        loss = 1 - 0\n",
    "        return {'loss': loss, 'status': STATUS_OK}\n",
    "\n",
    "    except Exception as obj:\n",
    "        #print(\"Exception: \",obj)\n",
    "        loss = 1 - 0\n",
    "        return {'loss': loss, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(\n",
    "             trials, \n",
    "             max_evals, \n",
    "             random_state=RANDOM_SEED):\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    This is the optimization function that given a space (space here) of \n",
    "    hyperparameters and a scoring function (score here), finds the best hyperparameters.\n",
    "    \"\"\"\n",
    "    # To learn more about XGBoost parameters, head to this page: \n",
    "    # https://github.com/dmlc/xgboost/blob/master/doc/parameter.md\n",
    "    space = {\n",
    "        'n_estimators': hp.quniform('n_estimators', 100, 300, 1),\n",
    "        'eta': hp.quniform('eta', 0.025, 0.5, 0.025),\n",
    "        # A problem with max_depth casted to float instead of int with\n",
    "        # the hp.quniform method.\n",
    "        'max_depth':  hp.choice('max_depth', np.arange(1, 7, dtype=int)),\n",
    "        'min_child_weight': hp.quniform('min_child_weight', 1, 6, 1),\n",
    "        'subsample': hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "        'gamma': hp.quniform('gamma', 0, 1, 0.05),\n",
    "        'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n",
    "        'scale_pos_weight': hp.quniform('scale_pos_weight', 1,4, 0.05),\n",
    "        \"reg_alpha\": hp.quniform('reg_alpha', 0, 1, 0.05),\n",
    "        \"reg_lambda\": hp.quniform('reg_lambda', 1, 5, 0.05),\n",
    "        'eval_metric': 'logloss',\n",
    "        'objective': 'binary:logistic',\n",
    "        # Increase this number if you have more cores. Otherwise, remove it and it will default \n",
    "        # to the maxium number. \n",
    "        'nthread': 4,\n",
    "        'booster': 'gbtree',\n",
    "        'tree_method': 'exact',\n",
    "        'silent': 1,\n",
    "        'seed': random_state\n",
    "    }\n",
    "    # Use the fmin function from Hyperopt to find the best hyperparameters\n",
    "    best = fmin(score, \n",
    "                space, \n",
    "                algo=tpe.suggest, \n",
    "                trials=trials, \n",
    "                max_evals=max_evals)\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with params:                                                                                                  \n",
      "{'booster': 'gbtree', 'colsample_bytree': 0.75, 'eta': 0.07500000000000001, 'eval_metric': 'logloss', 'gamma': 0.5, 'max_depth': 2, 'min_child_weight': 2.0, 'n_estimators': 226.0, 'nthread': 4, 'objective': 'binary:logistic', 'reg_alpha': 0.25, 'reg_lambda': 4.4, 'scale_pos_weight': 3.85, 'seed': 1, 'silent': 1, 'subsample': 0.65, 'tree_method': 'exact'}\n",
      "[20:11:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541:                   \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "\tScore 0.8649667709892149                                                                                              \n",
      "\n",
      "\n",
      "Training with params:                                                                                                  \n",
      "{'booster': 'gbtree', 'colsample_bytree': 0.6000000000000001, 'eta': 0.35000000000000003, 'eval_metric': 'logloss', 'gamma': 0.30000000000000004, 'max_depth': 2, 'min_child_weight': 6.0, 'n_estimators': 138.0, 'nthread': 4, 'objective': 'binary:logistic', 'reg_alpha': 0.05, 'reg_lambda': 4.3500000000000005, 'scale_pos_weight': 3.1500000000000004, 'seed': 1, 'silent': 1, 'subsample': 0.7000000000000001, 'tree_method': 'exact'}\n",
      "[20:13:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541:                   \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "\tScore 0.8635799355981015                                                                                              \n",
      "\n",
      "\n",
      "Training with params:                                                                                                  \n",
      "{'booster': 'gbtree', 'colsample_bytree': 0.7000000000000001, 'eta': 0.35000000000000003, 'eval_metric': 'logloss', 'gamma': 0.55, 'max_depth': 2, 'min_child_weight': 3.0, 'n_estimators': 114.0, 'nthread': 4, 'objective': 'binary:logistic', 'reg_alpha': 0.9, 'reg_lambda': 2.45, 'scale_pos_weight': 3.5500000000000003, 'seed': 1, 'silent': 1, 'subsample': 0.8, 'tree_method': 'exact'}\n",
      "[20:14:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541:                   \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "\tScore 0.8628998285507433                                                                                              \n",
      "\n",
      "\n",
      "Training with params:                                                                                                  \n",
      "{'booster': 'gbtree', 'colsample_bytree': 0.55, 'eta': 0.07500000000000001, 'eval_metric': 'logloss', 'gamma': 0.5, 'max_depth': 4, 'min_child_weight': 2.0, 'n_estimators': 287.0, 'nthread': 4, 'objective': 'binary:logistic', 'reg_alpha': 0.45, 'reg_lambda': 1.4000000000000001, 'scale_pos_weight': 2.95, 'seed': 1, 'silent': 1, 'subsample': 0.65, 'tree_method': 'exact'}\n",
      "[20:15:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541:                   \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "\tScore 0.8684406753462284                                                                                              \n",
      "\n",
      "\n",
      "Training with params:                                                                                                  \n",
      "{'booster': 'gbtree', 'colsample_bytree': 0.8, 'eta': 0.275, 'eval_metric': 'logloss', 'gamma': 0.75, 'max_depth': 6, 'min_child_weight': 1.0, 'n_estimators': 126.0, 'nthread': 4, 'objective': 'binary:logistic', 'reg_alpha': 0.05, 'reg_lambda': 3.2, 'scale_pos_weight': 2.2, 'seed': 1, 'silent': 1, 'subsample': 0.65, 'tree_method': 'exact'}\n",
      "[20:18:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541:                   \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "\tScore 0.8620968248871823                                                                                              \n",
      "\n",
      "\n",
      "Training with params:                                                                                                  \n",
      "{'booster': 'gbtree', 'colsample_bytree': 0.8500000000000001, 'eta': 0.375, 'eval_metric': 'logloss', 'gamma': 0.7000000000000001, 'max_depth': 5, 'min_child_weight': 2.0, 'n_estimators': 228.0, 'nthread': 4, 'objective': 'binary:logistic', 'reg_alpha': 0.9500000000000001, 'reg_lambda': 3.0, 'scale_pos_weight': 1.5, 'seed': 1, 'silent': 1, 'subsample': 0.9, 'tree_method': 'exact'}\n",
      "[20:21:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541:                   \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "\tScore 0.8699248843509025                                                                                              \n",
      "\n",
      "\n",
      "Training with params:                                                                                                  \n",
      "{'booster': 'gbtree', 'colsample_bytree': 0.8500000000000001, 'eta': 0.15000000000000002, 'eval_metric': 'logloss', 'gamma': 1.0, 'max_depth': 2, 'min_child_weight': 4.0, 'n_estimators': 189.0, 'nthread': 4, 'objective': 'binary:logistic', 'reg_alpha': 0.0, 'reg_lambda': 1.25, 'scale_pos_weight': 2.2, 'seed': 1, 'silent': 1, 'subsample': 0.7000000000000001, 'tree_method': 'exact'}\n",
      "[20:25:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541:                   \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "\tScore 0.8686194069313429                                                                                              \n",
      "\n",
      "\n",
      "Training with params:                                                                                                  \n",
      "{'booster': 'gbtree', 'colsample_bytree': 0.55, 'eta': 0.15000000000000002, 'eval_metric': 'logloss', 'gamma': 0.4, 'max_depth': 6, 'min_child_weight': 6.0, 'n_estimators': 173.0, 'nthread': 4, 'objective': 'binary:logistic', 'reg_alpha': 0.6000000000000001, 'reg_lambda': 1.9500000000000002, 'scale_pos_weight': 3.45, 'seed': 1, 'silent': 1, 'subsample': 0.55, 'tree_method': 'exact'}\n",
      "[20:27:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541:                   \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "\tScore 0.861205365379128                                                                                               \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with params:                                                                                                  \n",
      "{'booster': 'gbtree', 'colsample_bytree': 0.6000000000000001, 'eta': 0.325, 'eval_metric': 'logloss', 'gamma': 0.9, 'max_depth': 3, 'min_child_weight': 4.0, 'n_estimators': 147.0, 'nthread': 4, 'objective': 'binary:logistic', 'reg_alpha': 0.0, 'reg_lambda': 3.45, 'scale_pos_weight': 3.25, 'seed': 1, 'silent': 1, 'subsample': 0.8, 'tree_method': 'exact'}\n",
      "[20:30:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541:                   \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "\tScore 0.8636002822279824                                                                                              \n",
      "\n",
      "\n",
      "Training with params:                                                                                                  \n",
      "{'booster': 'gbtree', 'colsample_bytree': 0.75, 'eta': 0.42500000000000004, 'eval_metric': 'logloss', 'gamma': 0.05, 'max_depth': 4, 'min_child_weight': 6.0, 'n_estimators': 253.0, 'nthread': 4, 'objective': 'binary:logistic', 'reg_alpha': 0.9500000000000001, 'reg_lambda': 1.8, 'scale_pos_weight': 3.3000000000000003, 'seed': 1, 'silent': 1, 'subsample': 0.65, 'tree_method': 'exact'}\n",
      "[20:31:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541:                   \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "\tScore 0.8594491755286882                                                                                              \n",
      "\n",
      "\n",
      "Training with params:                                                                                                  \n",
      "{'booster': 'gbtree', 'colsample_bytree': 0.8500000000000001, 'eta': 0.325, 'eval_metric': 'logloss', 'gamma': 0.0, 'max_depth': 5, 'min_child_weight': 2.0, 'n_estimators': 231.0, 'nthread': 4, 'objective': 'binary:logistic', 'reg_alpha': 0.5, 'reg_lambda': 1.5, 'scale_pos_weight': 2.6500000000000004, 'seed': 1, 'silent': 1, 'subsample': 0.8, 'tree_method': 'exact'}\n",
      "[20:35:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541:                   \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "\tScore 0.8680449710855795                                                                                              \n",
      "\n",
      "\n",
      "Training with params:                                                                                                  \n",
      "{'booster': 'gbtree', 'colsample_bytree': 0.6000000000000001, 'eta': 0.375, 'eval_metric': 'logloss', 'gamma': 0.8500000000000001, 'max_depth': 4, 'min_child_weight': 3.0, 'n_estimators': 238.0, 'nthread': 4, 'objective': 'binary:logistic', 'reg_alpha': 0.9500000000000001, 'reg_lambda': 4.05, 'scale_pos_weight': 3.2, 'seed': 1, 'silent': 1, 'subsample': 0.9500000000000001, 'tree_method': 'exact'}\n",
      "[20:39:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541:                   \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "\tScore 0.8625829796185409                                                                                              \n",
      "\n",
      "\n",
      "Training with params:                                                                                                  \n",
      "{'booster': 'gbtree', 'colsample_bytree': 0.9500000000000001, 'eta': 0.225, 'eval_metric': 'logloss', 'gamma': 0.30000000000000004, 'max_depth': 5, 'min_child_weight': 1.0, 'n_estimators': 142.0, 'nthread': 4, 'objective': 'binary:logistic', 'reg_alpha': 0.45, 'reg_lambda': 4.55, 'scale_pos_weight': 2.3000000000000003, 'seed': 1, 'silent': 1, 'subsample': 1.0, 'tree_method': 'exact'}\n",
      "[20:43:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541:                   \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "\tScore 0.8620920373879238                                                                                              \n",
      "\n",
      "\n",
      "Training with params:                                                                                                  \n",
      "{'booster': 'gbtree', 'colsample_bytree': 0.55, 'eta': 0.225, 'eval_metric': 'logloss', 'gamma': 0.30000000000000004, 'max_depth': 2, 'min_child_weight': 2.0, 'n_estimators': 256.0, 'nthread': 4, 'objective': 'binary:logistic', 'reg_alpha': 0.25, 'reg_lambda': 4.2, 'scale_pos_weight': 3.5, 'seed': 1, 'silent': 1, 'subsample': 0.75, 'tree_method': 'exact'}\n",
      "[20:46:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541:                   \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "\tScore 0.8615363260830062                                                                                              \n",
      "\n",
      "\n",
      "Training with params:                                                                                                  \n",
      "{'booster': 'gbtree', 'colsample_bytree': 0.55, 'eta': 0.4, 'eval_metric': 'logloss', 'gamma': 0.35000000000000003, 'max_depth': 5, 'min_child_weight': 2.0, 'n_estimators': 183.0, 'nthread': 4, 'objective': 'binary:logistic', 'reg_alpha': 0.65, 'reg_lambda': 1.55, 'scale_pos_weight': 3.6, 'seed': 1, 'silent': 1, 'subsample': 0.8500000000000001, 'tree_method': 'exact'}\n",
      "[20:48:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541:                   \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "\tScore 0.865837844850157                                                                                               \n",
      "\n",
      "\n",
      "Training with params:                                                                                                  \n",
      "{'booster': 'gbtree', 'colsample_bytree': 0.7000000000000001, 'eta': 0.375, 'eval_metric': 'logloss', 'gamma': 0.4, 'max_depth': 2, 'min_child_weight': 5.0, 'n_estimators': 240.0, 'nthread': 4, 'objective': 'binary:logistic', 'reg_alpha': 0.05, 'reg_lambda': 2.0, 'scale_pos_weight': 1.6500000000000001, 'seed': 1, 'silent': 1, 'subsample': 1.0, 'tree_method': 'exact'}\n",
      "[20:51:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541:                   \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "\tScore 0.8644317914609796                                                                                              \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with params:                                                                                                  \n",
      "{'booster': 'gbtree', 'colsample_bytree': 0.8, 'eta': 0.45, 'eval_metric': 'logloss', 'gamma': 0.05, 'max_depth': 5, 'min_child_weight': 5.0, 'n_estimators': 254.0, 'nthread': 4, 'objective': 'binary:logistic', 'reg_alpha': 0.1, 'reg_lambda': 4.4, 'scale_pos_weight': 2.9000000000000004, 'seed': 1, 'silent': 1, 'subsample': 0.5, 'tree_method': 'exact'}\n",
      "[20:53:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541:                   \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "\tScore 0.8605400168461849                                                                                              \n",
      "\n",
      "\n",
      "Training with params:                                                                                                  \n",
      "{'booster': 'gbtree', 'colsample_bytree': 1.0, 'eta': 0.07500000000000001, 'eval_metric': 'logloss', 'gamma': 0.35000000000000003, 'max_depth': 2, 'min_child_weight': 3.0, 'n_estimators': 183.0, 'nthread': 4, 'objective': 'binary:logistic', 'reg_alpha': 0.2, 'reg_lambda': 3.9000000000000004, 'scale_pos_weight': 3.6, 'seed': 1, 'silent': 1, 'subsample': 0.55, 'tree_method': 'exact'}\n",
      "[20:57:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541:                   \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "\tScore 0.8700582101107058                                                                                              \n",
      "\n",
      "\n",
      "Training with params:                                                                                                  \n",
      "{'booster': 'gbtree', 'colsample_bytree': 0.8, 'eta': 0.25, 'eval_metric': 'logloss', 'gamma': 0.4, 'max_depth': 2, 'min_child_weight': 6.0, 'n_estimators': 154.0, 'nthread': 4, 'objective': 'binary:logistic', 'reg_alpha': 0.4, 'reg_lambda': 4.3, 'scale_pos_weight': 1.55, 'seed': 1, 'silent': 1, 'subsample': 0.65, 'tree_method': 'exact'}\n",
      "[20:58:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541:                   \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "\tScore 0.8704503328749867                                                                                              \n",
      "\n",
      "\n",
      "Training with params:                                                                                                  \n",
      "{'booster': 'gbtree', 'colsample_bytree': 0.6000000000000001, 'eta': 0.35000000000000003, 'eval_metric': 'logloss', 'gamma': 0.2, 'max_depth': 2, 'min_child_weight': 4.0, 'n_estimators': 241.0, 'nthread': 4, 'objective': 'binary:logistic', 'reg_alpha': 0.75, 'reg_lambda': 3.25, 'scale_pos_weight': 3.5, 'seed': 1, 'silent': 1, 'subsample': 0.7000000000000001, 'tree_method': 'exact'}\n",
      "[21:00:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541:                   \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "\tScore 0.8554525665433481                                                                                              \n",
      "\n",
      "\n",
      "Training with params:                                                                                                  \n",
      "{'booster': 'gbtree', 'colsample_bytree': 1.0, 'eta': 0.15000000000000002, 'eval_metric': 'logloss', 'gamma': 0.2, 'max_depth': 1, 'min_child_weight': 3.0, 'n_estimators': 160.0, 'nthread': 4, 'objective': 'binary:logistic', 'reg_alpha': 0.25, 'reg_lambda': 4.9, 'scale_pos_weight': 1.25, 'seed': 1, 'silent': 1, 'subsample': 0.55, 'tree_method': 'exact'}\n",
      "[21:01:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541:                   \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "\tScore 0.8689837411015897                                                                                              \n",
      "\n",
      "\n",
      "Training with params:                                                                                                  \n",
      "{'booster': 'gbtree', 'colsample_bytree': 0.9500000000000001, 'eta': 0.05, 'eval_metric': 'logloss', 'gamma': 0.6000000000000001, 'max_depth': 1, 'min_child_weight': 3.0, 'n_estimators': 199.0, 'nthread': 4, 'objective': 'binary:logistic', 'reg_alpha': 0.30000000000000004, 'reg_lambda': 3.7, 'scale_pos_weight': 1.8, 'seed': 1, 'silent': 1, 'subsample': 0.55, 'tree_method': 'exact'}\n",
      "[21:02:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541:                   \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "\tScore 0.8676375887893233                                                                                              \n",
      "\n",
      "\n",
      "Training with params:                                                                                                  \n",
      "{'booster': 'gbtree', 'colsample_bytree': 1.0, 'eta': 0.5, 'eval_metric': 'logloss', 'gamma': 0.15000000000000002, 'max_depth': 3, 'min_child_weight': 5.0, 'n_estimators': 104.0, 'nthread': 4, 'objective': 'binary:logistic', 'reg_alpha': 0.35000000000000003, 'reg_lambda': 4.95, 'scale_pos_weight': 1.0, 'seed': 1, 'silent': 1, 'subsample': 0.6000000000000001, 'tree_method': 'exact'}\n",
      "[21:03:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541:                   \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "\tScore 0.8663878231066435                                                                                              \n",
      "\n",
      "\n",
      "Training with params:                                                                                                  \n",
      "{'booster': 'gbtree', 'colsample_bytree': 0.9, 'eta': 0.2, 'eval_metric': 'logloss', 'gamma': 0.65, 'max_depth': 2, 'min_child_weight': 4.0, 'n_estimators': 213.0, 'nthread': 4, 'objective': 'binary:logistic', 'reg_alpha': 0.15000000000000002, 'reg_lambda': 3.8000000000000003, 'scale_pos_weight': 1.9000000000000001, 'seed': 1, 'silent': 1, 'subsample': 0.5, 'tree_method': 'exact'}\n",
      "[21:05:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541:                   \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "\tScore 0.8612410591785071                                                                                              \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with params:                                                                                                  \n",
      "{'booster': 'gbtree', 'colsample_bytree': 0.9500000000000001, 'eta': 0.1, 'eval_metric': 'logloss', 'gamma': 0.45, 'max_depth': 1, 'min_child_weight': 5.0, 'n_estimators': 163.0, 'nthread': 4, 'objective': 'binary:logistic', 'reg_alpha': 0.4, 'reg_lambda': 4.800000000000001, 'scale_pos_weight': 1.05, 'seed': 1, 'silent': 1, 'subsample': 0.6000000000000001, 'tree_method': 'exact'}\n",
      "[21:07:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541:                   \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "\tScore 0.8631893708366808                                                                                              \n",
      "\n",
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [56:24<00:00, 135.40s/trial, best loss: 0.12954966712501326]\n",
      "The best hyperparameters are:  \n",
      "\n",
      "{'colsample_bytree': 0.8, 'eta': 0.25, 'gamma': 0.4, 'max_depth': 1, 'min_child_weight': 6.0, 'n_estimators': 154.0, 'reg_alpha': 0.4, 'reg_lambda': 4.3, 'scale_pos_weight': 1.55, 'subsample': 0.65}\n"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "MAX_EVALS = 25\n",
    "\n",
    "best_hyperparams = optimize(trials, MAX_EVALS)\n",
    "print(\"The best hyperparameters are: \", \"\\n\")\n",
    "print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.8,\n",
       " 'eta': 0.25,\n",
       " 'gamma': 0.4,\n",
       " 'max_depth': 1,\n",
       " 'min_child_weight': 6.0,\n",
       " 'n_estimators': 154.0,\n",
       " 'reg_alpha': 0.4,\n",
       " 'reg_lambda': 4.3,\n",
       " 'scale_pos_weight': 1.55,\n",
       " 'subsample': 0.65}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = best_hyperparams\n",
    "num_round = int(param['n_estimators'])\n",
    "del param['n_estimators']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_splits = 5\n",
    "skf = StratifiedKFold(n_splits= num_splits, random_state= RANDOM_SEED, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dxtest = xgb.DMatrix(XTest[selected_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20442, 1)\n",
      "FOLD ....  1\n",
      "TRAIN: [    0     1     2 ... 17518 17519 17520] TEST: [   10    12    19 ... 17508 17510 17511]\n",
      "[0]\ttrain-rmse:0.43137\teval-rmse:0.43109\n",
      "[50]\ttrain-rmse:0.28775\teval-rmse:0.28993\n",
      "[100]\ttrain-rmse:0.28410\teval-rmse:0.29027\n",
      "[150]\ttrain-rmse:0.28193\teval-rmse:0.29157\n",
      "[153]\ttrain-rmse:0.28171\teval-rmse:0.29168\n",
      "FOLD ....  2\n",
      "TRAIN: [    1     4     7 ... 17518 17519 17520] TEST: [    0     2     3 ... 17503 17513 17514]\n",
      "[0]\ttrain-rmse:0.43096\teval-rmse:0.43123\n",
      "[50]\ttrain-rmse:0.28743\teval-rmse:0.29352\n",
      "[100]\ttrain-rmse:0.28465\teval-rmse:0.29363\n",
      "[150]\ttrain-rmse:0.28231\teval-rmse:0.29424\n",
      "[153]\ttrain-rmse:0.28211\teval-rmse:0.29404\n",
      "FOLD ....  3\n",
      "TRAIN: [    0     1     2 ... 17515 17516 17519] TEST: [   20    25    26 ... 17517 17518 17520]\n",
      "[0]\ttrain-rmse:0.43087\teval-rmse:0.43070\n",
      "[50]\ttrain-rmse:0.28848\teval-rmse:0.28943\n",
      "[100]\ttrain-rmse:0.28496\teval-rmse:0.28873\n",
      "[150]\ttrain-rmse:0.28229\teval-rmse:0.28934\n",
      "[153]\ttrain-rmse:0.28229\teval-rmse:0.28974\n",
      "FOLD ....  4\n",
      "TRAIN: [    0     2     3 ... 17517 17518 17520] TEST: [    1     4     9 ... 17512 17516 17519]\n",
      "[0]\ttrain-rmse:0.43144\teval-rmse:0.43127\n",
      "[50]\ttrain-rmse:0.28718\teval-rmse:0.29292\n",
      "[100]\ttrain-rmse:0.28436\teval-rmse:0.29239\n",
      "[150]\ttrain-rmse:0.28201\teval-rmse:0.29295\n",
      "[153]\ttrain-rmse:0.28192\teval-rmse:0.29298\n",
      "FOLD ....  5\n",
      "TRAIN: [    0     1     2 ... 17518 17519 17520] TEST: [    7    11    14 ... 17500 17501 17515]\n",
      "[0]\ttrain-rmse:0.43100\teval-rmse:0.43140\n",
      "[50]\ttrain-rmse:0.28584\teval-rmse:0.29656\n",
      "[100]\ttrain-rmse:0.28340\teval-rmse:0.29709\n",
      "[150]\ttrain-rmse:0.28067\teval-rmse:0.29783\n",
      "[153]\ttrain-rmse:0.28070\teval-rmse:0.29790\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = np.zeros((XTest[selected_features].shape[0], 1))\n",
    "print(y_test_pred.shape)\n",
    "y_valid_scores = []\n",
    "\n",
    "X_TRAIN = X[selected_features].copy()\n",
    "Y_TRAIN = y.copy()\n",
    "X_TRAIN = X_TRAIN.reindex()\n",
    "Y_TRAIN = Y_TRAIN.reindex()\n",
    "\n",
    "fold_cnt = 1\n",
    "for train_index, test_index in skf.split(X_TRAIN,Y_TRAIN):\n",
    "    print(\"FOLD .... \",fold_cnt)\n",
    "    fold_cnt += 1\n",
    "    \n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_valid = X_TRAIN.iloc[train_index], X_TRAIN.iloc[test_index]\n",
    "    y_train, y_valid = Y_TRAIN.iloc[train_index], Y_TRAIN.iloc[test_index]\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "    \n",
    "    evallist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "\n",
    "    # Training xgb model\n",
    "    bst = xgb.train(param, dtrain, num_round, evallist, verbose_eval=50)\n",
    "    \n",
    "    # Predict Validation\n",
    "    y_pred_valid = bst.predict(dvalid, ntree_limit=bst.best_iteration + 1)\n",
    "    y_valid_scores.append(f1_score(y_valid, (y_pred_valid >= 0.5).astype(int), average='weighted'))\n",
    "   \n",
    "    # Predict Test \n",
    "    y_pred = bst.predict(dxtest, ntree_limit=bst.best_iteration+1)\n",
    "    \n",
    "    y_test_pred += y_pred.reshape(-1,1)\n",
    "\n",
    "#Normalize test predicted probability\n",
    "y_test_pred /= num_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.865517655523632,\n",
       " 0.8686321302688589,\n",
       " 0.8680522377481038,\n",
       " 0.8642049601940295,\n",
       " 0.8630601243049213]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation_score:  0.865893421607909\n"
     ]
    }
   ],
   "source": [
    "print(\"Average validation_score: \",np.mean(y_valid_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = df_test[['Col1']].copy()\n",
    "output['Col2'] = (y_test_pred >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Col1</th>\n",
       "      <th>Col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RIGD58ZWD</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RIH660YDS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RIH660Q96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RIYDO15W1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RIYBGC1ZD</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Col1  Col2\n",
       "0  RIGD58ZWD     0\n",
       "1  RIH660YDS     1\n",
       "2  RIH660Q96     0\n",
       "3  RIYDO15W1     0\n",
       "4  RIYBGC1ZD     0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    98.082379\n",
       "1     1.917621\n",
       "Name: Col2, dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['Col2'].value_counts()/output.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv(\"./predict_hdfc_xgb_oof.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.to_csv of             Col1  Col2\n",
      "0      RIGD58ZWD     0\n",
      "1      RIH660YDS     1\n",
      "2      RIH660Q96     0\n",
      "3      RIYDO15W1     0\n",
      "4      RIYBGC1ZD     0\n",
      "...          ...   ...\n",
      "20437   OL0I6O5R     0\n",
      "20438   OL0I65ZW     0\n",
      "20439   OL0I6CXW     0\n",
      "20440     O2VIUO     0\n",
      "20441   M7VKV5QD     0\n",
      "\n",
      "[20442 rows x 2 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(output.to_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
